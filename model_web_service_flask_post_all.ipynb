{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A fast and dirty intro to running a model via web service\n",
    "\n",
    "Have you ever needed to score data with a predictive model in (near) real time, using a remote server/machine? While not a statistical or machine learning topic, it's one that I struggled with for a while, and haven't found many resources for.\n",
    "\n",
    "In this post, we're going to assume an understanding of predictive modeling and python, to focus on a simple way to set up a web service that can recieve data and return model predictions over the internet.\n",
    "\n",
    "This way is by no means definitive, but is straightforward and effective."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "\n",
    "### [Part 1: Model](http://spencercarter.info/docs/model_web_service_flask_p1.ipynb)\n",
    "\n",
    "To get our model going, let's just fit a simple LASSO with K-fold on the boston data\n",
    "This is a quick model just to get something running in our web service. It's roughly based on [This example code from sklearn](http://scikit-learn.org/stable/auto_examples/feature_selection/plot_select_from_model_boston.html#sphx-glr-auto-examples-feature-selection-plot-select-from-model-boston-py). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting model...\n",
      "Model fit. R-Squared = 71.06%\n"
     ]
    }
   ],
   "source": [
    "import sklearn as sk\n",
    "import numpy as np\n",
    "from sklearn.datasets import load_boston\n",
    "from sklearn.linear_model import LassoCV\n",
    "import pandas as pd\n",
    "from flask import Flask\n",
    "from flask import request\n",
    "import requests\n",
    "import pickle\n",
    "from time import time\n",
    "\n",
    "# Load up the Mass data... I like keeping things in pandas DFs\n",
    "mass = load_boston()\n",
    "\n",
    "# Lowecase the predictor names and send the data to Pandas\n",
    "predictors = [var.lower() for var in mass.feature_names]\n",
    "X = pd.DataFrame(mass['data'], columns=predictors)\n",
    "y = pd.Series(mass['target']) # medv\n",
    "\n",
    "# Run a 3-fold CV LASSO with 10 weights chosen by steepest descent\n",
    "lasso = LassoCV(n_alphas=10, normalize=True, cv=3)\n",
    "\n",
    "print(\"Fitting model...\")\n",
    "lasso.fit(X,y)\n",
    "\n",
    "print(\"Model fit. R-Squared = {0:0.2f}%\".format(100*lasso.score(X,y)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is enough for our demonstration... We're assuming a basic familiarity with modeling concepts like variable selection and use of validation data.\n",
    "\n",
    "Now, we're going to save the current objects down to disk so our listener and sender can access them. While everything is together in this document, the source code I've made available is split into Model, Listener, and Sender"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# A couple simple pickle functions\n",
    "def pickle_me(obj, outfile):\n",
    "    with open(outfile, 'wb') as f:\n",
    "        pickle.dump(obj, f, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "    return None\n",
    "\n",
    "def unpickle_me(infile):\n",
    "    with open(infile, 'rb') as f:\n",
    "        unpickled = pickle.load(f)\n",
    "    return unpickled\n",
    "               \n",
    "to_pickle = {'X':X, 'lasso':lasso, 'dtypes':X.dtypes, 'predictors':predictors}\n",
    "pickle_me(to_pickle, 'model_web_service.pickle')    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "### [Part 2: Listener](http://spencercarter.info/docs/model_web_service_flask_2.ipynb)\n",
    "\n",
    "Okay, now we've got a model. To get out web service going, we're going to need two things:\n",
    "1. A scoring function to turn data to predictions\n",
    "2. A web service running on our local machine, listening for requests\n",
    "\n",
    "#### 2.1: Scoring Function\n",
    "Let's start with the scoring function. This function is going to assume that the data will come in as a JSON, which we're just going to use to pass data in the form:\n",
    "\n",
    "```\n",
    "{<variable1>:<value>, <variable2>:<value>, ... , <variableN>:<value>}\n",
    "```\n",
    "\n",
    "This should look familiar; simple JSONs look like a dict in python. Just about any data structure we may want to impose will work here, but I've chosen JSON because it's easy to receive in the web service with *flask.request*.get_json(), and pandas makes it simple to send/score with *DataFrame*.to_json() and *pandas*.read_json().\n",
    "\n",
    "To ensure variables are consistent with expectations, we're going to pass all the data recieved, but keep just the values that the model expects, and type them accordingly (object, int, float, etc.)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Repeat the setup block for the listener and sender\n",
    "import sklearn as sk\n",
    "import numpy as np\n",
    "from sklearn.datasets import load_boston\n",
    "from sklearn.linear_model import LassoCV\n",
    "import pandas as pd\n",
    "from flask import Flask\n",
    "from flask import request\n",
    "import requests\n",
    "import pickle\n",
    "from time import time\n",
    "\n",
    "def unpickle_me(infile):\n",
    "    with open(infile, 'rb') as f:\n",
    "        unpickled = pickle.load(f)\n",
    "    return unpickled\n",
    "\n",
    "# Read in our pickled dictionary\n",
    "stored = unpickle_me('model_web_service.pickle')\n",
    "\n",
    "# Read our data and model back into memory\n",
    "df_types = stored['dtypes']\n",
    "predictors = stored['predictors']\n",
    "X = stored['X']\n",
    "lasso = stored['lasso']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at what the JSON data will look like. Call *DataFrame*.to_json() on a slice of a DF."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'{\"crim\":0.00632,\"zn\":18.0,\"indus\":2.31,\"chas\":0.0,\"nox\":0.538,\"rm\":6.575,\"age\":65.2,\"dis\":4.09,\"rad\":1.0,\"tax\":296.0,\"ptratio\":15.3,\"b\":396.9,\"lstat\":4.98}'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.iloc[0].to_json()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, create a scoring function that takes a JSON, and returns a model prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Direct prediction: 30.505530469875787 \n",
      "Using this function: 30.505530469875787\n"
     ]
    }
   ],
   "source": [
    "def score_obs(indata, model=lasso, predictors=predictors, df_types=df_types):\n",
    "    \"\"\"\n",
    "    Scoring function. Takes a JSON of variables and values, then runs them through the model and returns a prediction\n",
    "    \"\"\"\n",
    "\n",
    "    # Structure the input into a DF, but then impose the expected variable structure, using the .loc slicer\n",
    "    data = pd.read_json(indata,typ='series').to_frame().T.loc[:,predictors]\n",
    "    \n",
    "    # .loc will drop excess, and create variables that aren't present. Fill their values so the model runs\n",
    "    data.fillna(0, inplace=True) # !!! In practice, you should actually build in error handling or imputation\n",
    "    \n",
    "    # Fix the dtypes\n",
    "    for c in data:\n",
    "        data[c] = data[c].astype(df_types[c])\n",
    "    \n",
    "    # predict returns an array, so grab the first value\n",
    "    return model.predict(data.values.reshape(1,-1))[0]\n",
    "\n",
    "# To make sure everything is working, compare a direct prediction against one from the function\n",
    "print(\"Direct prediction: {} \\nUsing this function: {}\".format(lasso.predict(X.iloc[0:1])[0],score_obs(X.iloc[0].to_json())))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Note on some hangups I've run into:*\n",
    "\n",
    "As you can see in the above, to_json() returns a string. To read a single observation with read_json(), we need to read it as a series, shaped (p,)\n",
    "\n",
    "```python\n",
    "pd.read_json(indata,typ='series')\n",
    "```\n",
    "\n",
    "To get back to the intended structure, we send that series to a DataFrame, and transpose to get the (1,p) shape.\n",
    "\n",
    "```python\n",
    ".to_frame().T\n",
    "```\n",
    "Now the model is in memory, and we have a scoring function! Let's set up the web service."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "#### 2.2: Web Service\n",
    "\n",
    "This is a simple [RESTful](https://en.wikipedia.org/wiki/Representational_state_transfer) webservice that accepts data via POST, and returns the model prediction.\n",
    "\n",
    "*Strictly speaking, model predictions may more closely match GET requests in REST, because they can be repeated without care and don't alter anything on the server (idempotent). However, I'm using POST because model data can get extremely lengthy, and might exceed what some broswers can handle in a URL. [Here's some more info about REST](http://blog.teamtreehouse.com/the-definitive-guide-to-get-vs-post).*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " * Running on http://127.0.0.1:1234/ (Press CTRL+C to quit)\n"
     ]
    }
   ],
   "source": [
    "app = Flask(__name__)\n",
    "\n",
    "app.silent = True # Suppressess logging and errors. In practice, you'll likely want them, or import logging\n",
    " \n",
    "@app.route(\"/mass\", methods=['POST'])\n",
    "def predict_medv():\n",
    "    \n",
    "    # Get the JSON from the request and send to our scoring function\n",
    "    score = score_obs(request.get_json()) \n",
    "    \n",
    "    return \"{0:0.2f}\".format(score)\n",
    "\n",
    "app.run(port=1234) # defaults to localhost. Use host= option to change"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's break it down:\n",
    "\n",
    "```python\n",
    "@app.route(\"/mass\", methods=['POST'])\n",
    "```\n",
    "\n",
    "Configured the the web service to run at ```<host>:<port>/mass``` and specifies that we'll be sending POST requests.\n",
    "\n",
    "```python\n",
    "def predict_medv():\n",
    "```\n",
    "\n",
    "Creates the function that will execute upon requests. And lastly,\n",
    "\n",
    "```python\n",
    "app.run(port=1234)\n",
    "```\n",
    "\n",
    "Starts up the web service on the ```<host>:<port>``` specified, defaulting host to localhost (127.0.0.1).\n",
    "\n",
    "At this point, the web service is up, and will keep the shell running, so if you're in a notebook, this will lock it up. In the raw code, I've broken out the listener from the sender so you can run both at the same time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [Part 3: Sender](http://spencercarter.info/docs/model_web_service_flask_3.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Home stretch! Now we just need a way to send data to our web service."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Repeat the setup block for the listener and sender\n",
    "import sklearn as sk\n",
    "import numpy as np\n",
    "from sklearn.datasets import load_boston\n",
    "from sklearn.linear_model import LassoCV\n",
    "import pandas as pd\n",
    "from flask import Flask\n",
    "from flask import request\n",
    "import requests\n",
    "import pickle\n",
    "from time import time\n",
    "\n",
    "def unpickle_me(infile):\n",
    "    with open(infile, 'rb') as f:\n",
    "        unpickled = pickle.load(f)\n",
    "    return unpickled\n",
    "\n",
    "# Read in our pickled dictionary\n",
    "stored = unpickle_me('model_web_service.pickle')\n",
    "\n",
    "# Save the data types we  expect, to ensure typing works\n",
    "df_types = stored['dtypes']\n",
    "predictors = stored['predictors']\n",
    "X = stored['X']\n",
    "lasso = stored['lasso']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to generate some test cases from our data. As you saw when creating the scoring function, we're going to turn our data into a JSON. To make things a little simpler here, we're going to make a sender that takes a row number, and sends a JSON of the data to our webservice (which you should run in a separate notebook).\n",
    "\n",
    "Status code 200 indicates success, and all others indicate something failed. As a simple failsafe, we'll return 0.0 to signal a failure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def make_a_post(indx):\n",
    "    url = \"http://127.0.0.1:1234/mass\"\n",
    "    data = X.iloc[indx].to_json()\n",
    "    req = requests.post(url, json=data)\n",
    "    if req.status_code != 200:\n",
    "        return .0\n",
    "    return float(req.content.decode(\"utf-8\")) # The prediction will come in as a byte. Decode, then make float"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, let's toss obervation 0 to the webservice. We know from earlier, that it should return 30.505530469875787, rounded to 2 decimal-places. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30.51"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "make_a_post(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And a few more for show. Let's sample over the observations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Obs 257: 41.79\n",
      "Obs 195: 37.82\n",
      "Obs 166: 37.75\n",
      "Obs 168: 26.51\n",
      "Obs 027: 16.4\n",
      "Obs 289: 26.16\n",
      "Obs 107: 20.25\n",
      "Obs 225: 38.77\n",
      "Obs 322: 23.24\n",
      "Obs 504: 26.59\n"
     ]
    }
   ],
   "source": [
    "for i in np.random.choice(range(X.shape[0]), 10):\n",
    "    print(\"Obs {0:03d}: {1}\".format(i, make_a_post(i)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And let's see how long these take to run. Take the average of 100 runs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Over 100 runs, the web service averaged 0.0114 seconds to score\n"
     ]
    }
   ],
   "source": [
    "lag = []\n",
    "rep = 100\n",
    "for _ in range(100):\n",
    "    t0 = time()\n",
    "    make_a_post(0)\n",
    "    lag.append(time()-t0)\n",
    "print(\"Over {0} runs, the web service averaged {1:0.4f} seconds to score\".format(rep,np.mean(lag)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Success!**\n",
    "\n",
    "Sky's the limit here. This post focused only on cases where you might stream individual data. However, Flask can also receive files, allowing us to send a CSV containing multiple observations to score. This might be a topic for a future post.\n",
    "\n",
    "To learn more about flask, check out the <a href=\"http://flask.pocoo.org/\">Official Documentation</a> and the <a href=\"https://pythonprogramming.net/practical-flask-introduction/\">pythonprogramming.net tutorial</a>."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base]",
   "language": "python",
   "name": "conda-env-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
