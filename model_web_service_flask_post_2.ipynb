{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A fast and dirty intro to running a model via web service\n",
    "\n",
    "Have you ever needed to score data with a predictive model in (near) real time, using a remote server/machine? While not a statistical or machine learning topic, it's one that I struggled with for a while, and haven't found many resources for.\n",
    "\n",
    "In this post, we're going to assume an understanding of predictive modeling and python, to focus on a simple way to set up a web service that can recieve data and return model predictions over the internet.\n",
    "\n",
    "This way is by no means definitive, but is straightforward and effective."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "### [Part 2: Listener](http://spencercarter.info/docs/model_web_service_flask_2.ipynb)\n",
    "\n",
    "Okay, now we've got a model. To get out web service going, we're going to need two things:\n",
    "1. A scoring function to turn data to predictions\n",
    "2. A web service running on our local machine, listening for requests\n",
    "\n",
    "#### 2.1: Scoring Function\n",
    "Let's start with the scoring function. This function is going to assume that the data will come in as a JSON, which we're just going to use to pass data in the form:\n",
    "\n",
    "```\n",
    "{<variable1>:<value>, <variable2>:<value>, ... , <variableN>:<value>}\n",
    "```\n",
    "\n",
    "This should look familiar; simple JSONs look like a dict in python. Just about any data structure we may want to impose will work here, but I've chosen JSON because it's easy to receive in the web service with *flask.request*.get_json(), and pandas makes it simple to send/score with *DataFrame*.to_json() and *pandas*.read_json().\n",
    "\n",
    "To ensure variables are consistent with expectations, we're going to pass all the data recieved, but keep just the values that the model expects, and type them accordingly (object, int, float, etc.)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Repeat the setup block for the listener and sender\n",
    "import sklearn as sk\n",
    "import numpy as np\n",
    "from sklearn.datasets import load_boston\n",
    "from sklearn.linear_model import LassoCV\n",
    "import pandas as pd\n",
    "from flask import Flask\n",
    "from flask import request\n",
    "import requests\n",
    "import pickle\n",
    "from time import time\n",
    "\n",
    "def unpickle_me(infile):\n",
    "    with open(infile, 'rb') as f:\n",
    "        unpickled = pickle.load(f)\n",
    "    return unpickled\n",
    "\n",
    "# Read in our pickled dictionary\n",
    "stored = unpickle_me('model_web_service.pickle')\n",
    "\n",
    "# Read our data and model back into memory\n",
    "df_types = stored['dtypes']\n",
    "predictors = stored['predictors']\n",
    "X = stored['X']\n",
    "lasso = stored['lasso']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at what the JSON data will look like. Call *DataFrame*.to_json() on a slice of a DF."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'{\"crim\":0.00632,\"zn\":18.0,\"indus\":2.31,\"chas\":0.0,\"nox\":0.538,\"rm\":6.575,\"age\":65.2,\"dis\":4.09,\"rad\":1.0,\"tax\":296.0,\"ptratio\":15.3,\"b\":396.9,\"lstat\":4.98}'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.iloc[0].to_json()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, create a scoring function that takes a JSON, and returns a model prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Direct prediction: 30.505530469875787 \n",
      "Using this function: 30.505530469875787\n"
     ]
    }
   ],
   "source": [
    "def score_obs(indata, model=lasso, predictors=predictors, df_types=df_types):\n",
    "    \"\"\"\n",
    "    Scoring function. Takes a JSON of variables and values, then runs them through the model and returns a prediction\n",
    "    \"\"\"\n",
    "\n",
    "    # Structure the input into a DF, but then impose the expected variable structure, using the .loc slicer\n",
    "    data = pd.read_json(indata,typ='series').to_frame().T.loc[:,predictors]\n",
    "    \n",
    "    # .loc will drop excess, and create variables that aren't present. Fill their values so the model runs\n",
    "    data.fillna(0, inplace=True) # !!! In practice, you should actually build in error handling or imputation\n",
    "    \n",
    "    # Fix the dtypes\n",
    "    for c in data:\n",
    "        data[c] = data[c].astype(df_types[c])\n",
    "    \n",
    "    # predict returns an array, so grab the first value\n",
    "    return model.predict(data.values.reshape(1,-1))[0]\n",
    "\n",
    "# To make sure everything is working, compare a direct prediction against one from the function\n",
    "print(\"Direct prediction: {} \\nUsing this function: {}\".format(lasso.predict(X.iloc[0:1])[0],score_obs(X.iloc[0].to_json())))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Note on some hangups I've run into:*\n",
    "\n",
    "As you can see in the above, to_json() returns a string. To read a single observation with read_json(), we need to read it as a series, shaped (p,)\n",
    "\n",
    "```python\n",
    "pd.read_json(indata,typ='series')\n",
    "```\n",
    "\n",
    "To get back to the intended structure, we send that series to a DataFrame, and transpose to get the (1,p) shape.\n",
    "\n",
    "```python\n",
    ".to_frame().T\n",
    "```\n",
    "Now the model is in memory, and we have a scoring function! Let's set up the web service."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "#### 2.2: Web Service\n",
    "\n",
    "This is a simple [RESTful](https://en.wikipedia.org/wiki/Representational_state_transfer) webservice that accepts data via POST, and returns the model prediction.\n",
    "\n",
    "*Strictly speaking, model predictions may more closely match GET requests in REST, because they can be repeated without care and don't alter anything on the server (idempotent). However, I'm using POST because model data can get extremely lengthy, and might exceed what some broswers can handle in a URL. [Here's some more info about REST](http://blog.teamtreehouse.com/the-definitive-guide-to-get-vs-post).*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "app = Flask(__name__)\n",
    "\n",
    "app.silent = True # Suppressess logging and errors. In practice, you'll likely want them, or import logging\n",
    "\n",
    "@app.route(\"/mass\", methods=['POST'])\n",
    "def predict_medv():\n",
    "    \n",
    "    # Get the JSON from the request and send to our scoring function\n",
    "    score = score_obs(request.get_json()) \n",
    "    \n",
    "    return \"{0:0.2f}\".format(score)\n",
    "\n",
    "app.run(port=1234) # defaults to localhost. Use host= option to change"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's break it down:\n",
    "\n",
    "```python\n",
    "@app.route(\"/mass\", methods=['POST'])\n",
    "```\n",
    "\n",
    "Configured the the web service to run at ```<host>:<port>/mass``` and specifies that we'll be sending POST requests.\n",
    "\n",
    "```python\n",
    "def predict_medv():\n",
    "```\n",
    "\n",
    "Creates the function that will execute upon requests. And lastly,\n",
    "\n",
    "```python\n",
    "app.run(port=1234)\n",
    "```\n",
    "\n",
    "Starts up the web service on the ```<host>:<port>``` specified, defaulting host to localhost (127.0.0.1).\n",
    "\n",
    "At this point, the web service is up, and will keep the shell running, so if you're in a notebook, this will lock it up. In the raw code, I've broken out the listener from the sender so you can run both at the same time."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base]",
   "language": "python",
   "name": "conda-env-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
